import numpy as np
import matplotlib.pyplot as plt
# Various functions to help generate some plots and 'see' gradient descent in action

def plot_contours_with_path(f, history, x_range=(-3,3), y_range=(-3,3), levels=30):
    xs = np.linspace(x_range[0], x_range[1], 400)
    ys = np.linspace(y_range[0], y_range[1], 400)
    X, Y = np.meshgrid(xs, ys)
    Z = np.vectorize(lambda a,b: f(np.array([a,b])))(X, Y)
    plt.figure(figsize=(6,5))
    CS = plt.contour(X, Y, Z, levels=levels)
    plt.clabel(CS, inline=True, fontsize=8)
    if history:
        xs_path = [h['x'] for h in history] + [history[-1]['x']]
        ys_path = [h['y'] for h in history] + [history[-1]['y']]
        plt.plot(xs_path, ys_path, marker='o')
    plt.xlabel('x'); plt.ylabel('y')
    plt.title(f"GD Path: {f_name}")
    plt.show()

def plot_scalar_vs_iter(history, key, title):
    plt.figure(figsize=(6,4))
    iters = [h['iter'] for h in history]
    vals = [h[key] for h in history]
    plt.plot(iters, vals)
    plt.xlabel('Iteration'); plt.ylabel(key)
    plt.title(title)
    plt.grid(True)
    plt.show()
def gradient_descent(f, v: np.ndarray, alpha: float, max_iterations: int, grad_bound: float):
    #print(v)
    history = []
    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))
        v_new = v - alpha*g
        #print(v_new)
        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def gradient_descent_momentum(f, v: np.ndarray, alpha: float, beta: float, max_iterations: int, grad_bound: float):
    history = []
    vel = np.zeros_like(v)
    for k in range(max_iterations):
        g = approx_grad(v)
        vel = beta * vel + alpha * g
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))
        v_new = v - vel
        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha, 'beta': beta})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def gradient_descent_accelerated_gradient(f, v: np.ndarray, alpha: float, beta: float, max_iterations: int, grad_bound: float):
    history = []
    vel = np.zeros_like(v)
    for k in range(max_iterations):
        w = v - beta * vel
        g = approx_grad(w)
        vel = beta * vel + alpha * g
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))
        v_new = v - vel
        history.append({
            'iter': k, 'x': v[0], 'y': v[1], 'f': f_v,
            'grad_norm': g_norm, 'alpha': alpha, 'beta': beta
        })
        v = v_new
        if g_norm < grad_bound:
            break

def f(v: np.ndarray):
    x, y = v
    #return x**4 + y**4 - 2*x*y + 6
    #return x**2 + 2*y**2
    return (1-x)**2 + (y-x**2)**2
#f_name = 'x**2 + 2*y**2'
#f_name = '(1-x)^2 + (y-x^2)^2'

def approx_grad(v: np.ndarray, h = 1e-10):
    """
    Computes approximations of f_x(a,b) and f_y(a,b) via
    f_x(a,b) ~ (f(a+h,b)-f(a-h,b))/(2h)
    f_y(a,b) ~ (f(a,b+h)-f(a-h,b))/(2h)

    returns [f_x(a,b),f_y(a,b)]
    """
    g = np.zeros_like(v) #Initialize g as zero vector
    for i in range(len(v)):
        step = np.zeros_like(v, dtype=float)
        step[i] = h
        #Step is of the form (0,0,..,0,h,0,...,0) with h in the i index
        g[i] = (f(v+step) - f(v-step))/(2*h) #Symmetric Approximation
    return g
v0 = np.array([4,2]) #Initial Point
alpha = 0.01 #Learning Rate
beta = 0.8 #Momentum Rate
max_iterations = 1000
grad_bound = 1e-6

result = gradient_descent(f, v0, alpha, max_iterations, grad_bound)

v_output, f_output = result['v_opt'], result['f_opt']
history = result['history']
print(f"v* = {v_output}, f(v*) = {f_output}")
print(f"Iterations: {len(history)}")
v0 = np.array([2,-1]) #Initial Point
alpha = 0.01 #Learning Rate
beta = 0.7 #Momentum Rate
max_iterations = 200
grad_bound = 1e-6

result = gradient_descent_momentum(f, v0, alpha, beta, max_iterations, grad_bound)

v_output, f_output = result['v_opt'], result['f_opt']
history = result['history']
print(f"v* = {v_output}, f(v*) = {f_output}")
print(f"Iterations: {len(history)}")
v0 = np.array([2,2]) #Initial Point
alpha = 0.01 #Learning Rate
beta = 0.7 #Momentum Rate
max_iterations = 200
grad_bound = 1e-6

result = gradient_descent_accelerated_gradient(f, v0, alpha, beta, max_iterations, grad_bound)

v_output, f_output = result['v_opt'], result['f_opt']
history = result['history']
print(f"v* = {v_output}, f(v*) = {f_output}")
print(f"Iterations: {len(history)}")
plot_contours_with_path(f, history, x_range=(-3,3), y_range=(-3,3), levels=50)
plot_scalar_vs_iter(history, 'f', 'Objective value vs iteration')
plot_scalar_vs_iter(history, 'grad_norm', 'Gradient norm vs iteration')
##########################################
# Play with different starting points.
##########################################

alpha = 0.01 #Learning Rate
beta = 0.0 #Momentum Rate (if beta = 0 we have regular gradient descent)
max_iterations = 1000
grad_bound = 1e-6

starting_points = [np.array([i,j]) for i in range(0,5) for j in range(-1,3)]
for v0 in starting_points:
    result = gradient_descent_momentum(f, v0, alpha, beta, max_iterations, grad_bound)
    #result = gradient_descent_accelerated_gradient(f, v0, alpha, beta, max_iterations, grad_bound)
    v_output, f_output = result['v_opt'], result['f_opt']
    history = result['history']
    print(f"v0 = {v0} -> v* = {v_output}, f(v*) = {f_output}")
    print(f"Iterations: {len(history)}")

    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}
