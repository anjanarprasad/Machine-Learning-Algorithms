#importing the libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# For splitting data and cross-validation
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
# For evaluation metrics
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, ConfusionMatrixDisplay, classification_report
)
#  Load the dataset
df = pd.read_csv("Loan_approval_data_2025.csv")
#checking for the rows and columns
print("Shape of the data (rows, columns):", df.shape)
print("\nFirst 5 rows:")# Showing the first 5 rows
display(df.head())

print("\nColumn info:")
print(df.info())
#how many 0s and 1s in the target variable
print("\nLoan status value counts (target variable):")
print(df["loan_status"].value_counts())
#Basic cleaning and feature selection

# Drop duplicate rows if any
df = df.drop_duplicates()

# dropping customer_id as a feature (it's just an ID) so not using it
df = df.drop(columns=["customer_id"])

# Separate input features X and target y
X = df.drop(columns=["loan_status"])#input features except the target
y = df["loan_status"]  # already 0 or 1, its the target variable
# 3. Trainâ€“test split

X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X,  #all input features
    y,  #target
    test_size=0.2,          # 80% training, 20% testing
    random_state=42,
    stratify=y              # keeps the class balance similar in train and test sets
)

print("Train shape:", X_train_raw.shape)
print("Test shape:", X_test_raw.shape)
# Simple preprocessing: handle numeric and categorical columns

# Numeric columns are non-object types (int, float)
categorical_cols = X_train_raw.select_dtypes(include=["object"]).columns.tolist()
# Categorical columns are of type 'object' (strings)
numeric_cols = X_train_raw.select_dtypes(exclude=["object"]).columns.tolist()

print("Numeric columns:", numeric_cols)
print("Categorical columns:", categorical_cols)
#One-hot encode categorical columns (turn categories into 0/1 columns)

#fitting  get_dummies on the TRAIN set, then align the TEST set to match.

#Creating  dummy variables for categorical features in train and test
# drop_first=True avoids perfect multicollinearity by dropping one level per category.
X_train_cat = pd.get_dummies(X_train_raw[categorical_cols], drop_first=True)
X_test_cat  = pd.get_dummies(X_test_raw[categorical_cols],  drop_first=True)

#Aligning  the dummy columns in train and test
# If some category appears only in train or only in test, this will add missing columns with zeros.
X_train_cat, X_test_cat = X_train_cat.align(X_test_cat, join="left", axis=1, fill_value=0)

print("Train categorical dummy shape:", X_train_cat.shape)
print("Test categorical dummy shape:", X_test_cat.shape)
#Working  with numeric columns and scale them

from sklearn.preprocessing import StandardScaler

# Get only the numeric columns from the raw training data
X_train_num = X_train_raw[numeric_cols].copy()
# Get only the numeric columns from the raw test data
X_test_num  = X_test_raw[numeric_cols].copy()

# Simple log transform   for skewed columns
for col in ["annual_income", "loan_amount"]:
    if col in X_train_num.columns:
        X_train_num[f"log_{col}"] = np.log1p(X_train_num[col])
        X_test_num[f"log_{col}"]  = np.log1p(X_test_num[col])

# Standardize numeric features (mean 0, std 1)
scaler = StandardScaler() # Create a scaler object
# Fit the scaler on the TRAIN numeric data and transform it
# .fit() learns the mean and standard deviation from X_train_num
# .transform() uses those learned values to scale the data
X_train_num_scaled = scaler.fit_transform(X_train_num)
# Use the already-fitted scaler to transform the TEST numeric data
X_test_num_scaled  = scaler.transform(X_test_num)

print("Numeric train shape after scaling:", X_train_num_scaled.shape)
print("Numeric test shape after scaling:", X_test_num_scaled.shape)
#Combine numeric and categorical features into final X_train and X_test

# Turn scaled numeric arrays back into DataFrames just for easier debugging
X_train_num_scaled_df = pd.DataFrame(
    X_train_num_scaled,# the scaled numeric values
    columns=X_train_num.columns,
    index=X_train_num.index
)

X_test_num_scaled_df = pd.DataFrame(
    X_test_num_scaled,# the scaled numeric values for the test set
    columns=X_train_num.columns,  # same columns as train numeric
    index=X_test_num.index
)

# Concatenate numeric and categorical side by side
# axis=1 means "join columns".
X_train_final = pd.concat([X_train_num_scaled_df, X_train_cat], axis=1)
#doing the same for test set
X_test_final  = pd.concat([X_test_num_scaled_df,  X_test_cat],  axis=1)

print("Final train feature shape:", X_train_final.shape)
print("Final test feature shape:", X_test_final.shape)
#Simple EDA (visualizations)

# Histograms for a few important numeric features
important_numeric = ["age", "annual_income", "credit_score", "loan_amount"]

for col in important_numeric:
    if col in df.columns:
        plt.figure()
        df[col].hist(bins=30)
        plt.title(f"Histogram of {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.show()

#Boxplot: annual_income vs loan_status
if "annual_income" in df.columns:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=df["loan_status"], y=df["annual_income"])
    plt.title("Annual income by loan status (0 = rejected, 1 = approved)")
    plt.xlabel("loan_status")
    plt.ylabel("annual_income")
    plt.show()

#Correlation heatmap for numeric features
plt.figure(figsize=(10, 8))
corr_matrix = df[numeric_cols + ["loan_status"]].corr()
sns.heatmap(corr_matrix, cmap="coolwarm", center=0)
plt.title("Correlation heatmap (numeric features + target)")
plt.show()
#two models: Logistic Regression and Random Forest

log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)
rf_clf  = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    n_jobs=-1
)

models = {
    "Logistic Regression": log_reg,
    "Random Forest": rf_clf
}
#K-Fold cross-validation on the training set

# using  5-fold Stratified K-Fold because this is classification.
# Split the training data into 5 folds
# Shuffle the data before splitting into folds
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, model in models.items():
    # Use F1-score as a main metric since classes are a bit imbalanced
    cv_scores = cross_val_score(
        model,
        X_train_final,
        y_train,
        cv=skf,
        scoring="f1"
    )
    print(f"{name}:")
    print(f"  F1 (5-fold mean) = {cv_scores.mean():.3f}")
    print(f"  F1 (std)         = {cv_scores.std():.3f}")
    print()
#Choose the better model and train on full training set based on results got from the test set, random forest is choosen

best_model = rf_clf

best_model.fit(X_train_final, y_train)

# Make predictions on the test set
y_pred = best_model.predict(X_test_final)
y_proba = best_model.predict_proba(X_test_final)[:, 1]

print("Done training the best model.")
#Final evaluation metrics on the test set

print("Classification report on test set:")
print(classification_report(y_test, y_pred))

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)

print(f"Accuracy : {acc:.3f}")
print(f"Precision: {prec:.3f}")
print(f"Recall   : {rec:.3f}")
print(f"F1-score : {f1:.3f}")

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title("Confusion Matrix - Best Model")
plt.show()
#Simple ROC curve and AUC

from sklearn.metrics import roc_auc_score, roc_curve

auc_score = roc_auc_score(y_test, y_proba)
print(f"ROC-AUC: {auc_score:.3f}")

fpr, tpr, thresholds = roc_curve(y_test, y_proba)

plt.figure()
plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auc_score:.3f})")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Best Model")
plt.legend()
plt.show()
