import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt
# Load dataset
DATA_PATH = "dataset_2191_sleep.csv.xls"

df = pd.read_csv(DATA_PATH)
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
df.head()
#Basic cleaning: replace "?" with NaN, convert to numeric
df = df.replace("?", np.nan)
df = df.apply(pd.to_numeric, errors="coerce")

# Drop rows with all NaNs
df = df.dropna(how="all").reset_index(drop=True)

df.info()
df.head()
#Define features and target


feature_cols = [
    "body_weight",
    "brain_weight",
    "max_life_span",
    "gestation_time",
    "predation_index",
    "sleep_exposure_index",
    "danger_index",
]

target_cont = "total_sleep"

# Drop rows where target is missing
df = df.dropna(subset=[target_cont]).reset_index(drop=True)

X = df[feature_cols]
y_cont = df[target_cont]

# Turn continuous total_sleep into 3 classes (low / medium / high)
y = pd.qcut(y_cont, q=3, labels=["low_sleep", "medium_sleep", "high_sleep"])

print("Class counts:\n", y.value_counts())
#Preprocessing: imputation + scaling (all numeric)

numeric_features = feature_cols

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features)
    ]
)

#Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
#SVM model with CV

svm_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("clf", SVC(kernel="rbf", C=1.0, gamma="scale"))
])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=cv, scoring="accuracy")

print("SVM CV accuracy scores:", svm_cv_scores)
print("SVM CV mean accuracy:", svm_cv_scores.mean())
# Fit SVM on full training set and evaluate on test set
svm_clf.fit(X_train, y_train)
y_pred_svm = svm_clf.predict(X_test)

print("SVM Test Accuracy:", accuracy_score(y_test, y_pred_svm))
print("SVM Test F1-score (macro):", f1_score(y_test, y_pred_svm, average="macro"))
print("\nSVM Classification report:\n", classification_report(y_test, y_pred_svm))
print("SVM Confusion matrix:\n", confusion_matrix(y_test, y_pred_svm))
#Decision Tree model with CV

tree_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("clf", DecisionTreeClassifier(max_depth=4, random_state=42))
])

tree_cv_scores = cross_val_score(tree_clf, X_train, y_train, cv=cv, scoring="accuracy")

print("Decision Tree CV accuracy scores:", tree_cv_scores)
print("Decision Tree CV mean accuracy:", tree_cv_scores.mean())
# Fit Decision Tree on full training set and evaluate on test set
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)

print("Decision Tree Test Accuracy:", accuracy_score(y_test, y_pred_tree))
print("Decision Tree Test F1-score (macro):", f1_score(y_test, y_pred_tree, average="macro"))
print("\nDecision Tree Classification report:\n", classification_report(y_test, y_pred_tree))
print("Decision Tree Confusion matrix:\n", confusion_matrix(y_test, y_pred_tree))
#Visualize SVM decision boundary in 2D using PCA

# First, preprocess & scale training data only
X_train_processed = preprocess.fit_transform(X_train)
X_test_processed  = preprocess.transform(X_test)

# PCA to 2 dimensions
pca = PCA(n_components=2, random_state=42)
X_train_2d = pca.fit_transform(X_train_processed)
X_test_2d  = pca.transform(X_test_processed)

# Train a new SVM on the 2D representation
svm_2d = SVC(kernel="rbf", C=1.0, gamma="scale")
svm_2d.fit(X_train_2d, y_train)

# Create meshgrid for boundary
x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1
y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1

xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 300),
    np.linspace(y_min, y_max, 300)
)

# Predict class labels on the grid
Z = svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Map string labels to integers for contourf
class_names = svm_2d.classes_              # e.g. ['high_sleep', 'low_sleep', 'medium_sleep']
label_to_int = {label: i for i, label in enumerate(class_names)}

Z_int = np.vectorize(label_to_int.get)(Z)  # same shape as Z, but numeric

plt.figure(figsize=(8, 6))
# Use numeric grid for contourf
contour = plt.contourf(xx, yy, Z_int, alpha=0.3)

# Also convert y_train to a NumPy array for masking
y_train_arr = np.array(y_train)

# Plot training points
for label in class_names:
    idx = (y_train_arr == label)
    plt.scatter(
        X_train_2d[idx, 0],
        X_train_2d[idx, 1],
        label=str(label),
        edgecolor="k",
        s=40
    )

# Plot support vectors
sv = svm_2d.support_vectors_
plt.scatter(
    sv[:, 0], sv[:, 1],
    s=80,
    facecolors="none",
    edgecolors="k",
    linewidths=1.5,
    label="Support Vectors"
)

plt.title("SVM Decision Boundary (PCA 2D)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.tight_layout()
plt.show()
#Visualize Decision Tree structure

# We need the fitted underlying tree, so we refit a "pure" tree on preprocessed data
tree_only = DecisionTreeClassifier(max_depth=4, random_state=42)
tree_only.fit(X_train_processed, y_train)

plt.figure(figsize=(18, 8))
plot_tree(
    tree_only,
    feature_names=[f"PC{i}" for i in range(X_train_processed.shape[1])] if X_train_processed.shape[1] > len(feature_cols) else feature_cols,
    class_names=tree_only.classes_,
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Structure")
plt.show()
