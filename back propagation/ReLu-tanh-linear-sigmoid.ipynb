import numpy as np
def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))
def dsigmoid(a): return a * (1.0 - a)

def tanh(z): return np.tanh(z)
def dtanh(a): return 1.0 - a**2

def relu(z): return np.maximum(0, z)
def drelu(a): return (a > 0).astype(float)

def elu(z): return np.where(z > 0, z, np.exp(z) - 1)
def delu(a): return np.where(a > 0, 1, np.exp(a))

def linear(z): return z
def dlinear(a): return np.ones_like(a)

activations = {
    "sigmoid": (sigmoid, dsigmoid),
    "tanh":    (tanh,    dtanh),
    "relu":    (relu,    drelu),
    "elu":     (elu,     delu),
    "linear":  (linear,  dlinear)
}

def mse_loss(a_L, y):
    return np.mean((a_L - y) ** 2)

# Function to initalize Ws and bs. We randomly select the Ws and set all of the bs to 0.
def init_params(layer_sizes, seed=0, scale=0.5):
    np.random.seed(seed)
    W = [np.random.randn(layer_sizes[l], layer_sizes[l - 1]) * scale
         for l in range(1, len(layer_sizes))]
    b = [np.zeros((layer_sizes[l], 1)) for l in range(1, len(layer_sizes))]

         def forward(x, W, b, act_funcs):
    a = x
    for l in range(len(W)):
        f, _ = activations[act_funcs[l]]
        z_l = W[l] @ a + b[l]
        a = f(z_l)
    return a

def forward_backward(x, y, W, b, act_funcs):
    """
    x: (n0, m)
    y: (nL, m)
    W[l]: (n_l, n_{l-1})
    b[l]: (n_l, 1)
    act_funcs: list of activation names, one per layer, e.g. ['sigmoid','tanh','relu','linear']
    Returns: loss_scalar, W_grads, b_grads
    """
    L = len(W) - 1
    a = []
    z = []

    # Forward Pass (build up a and z)
    for l in range(L + 1):
        f, _ = activations[act_funcs[l]]
        a_prev = x if l == 0 else a[l-1]
        z_l = W[l] @ a_prev + b[l]
        a_l = f(z_l)
        z.append(z_l)
        a.append(a_l)

    a_L = a[L]
    loss = mse_loss(a_L, y) #Later we will add in addtional loss functions

    # Backward Pass (generate delta, W_grads, b_grads)
    m = x.shape[1]
    deltas = [None] * (L + 1)
    W_grads = [None] * (L + 1)
    b_grads = [None] * (L + 1)

    # Output Layer (Layer L)
    _, df = activations[act_funcs[L]]
    deltas[L] = (a_L - y) * df(a_L)
    a_prev = x if L == 0 else a[L - 1]
    W_grads[L] = (deltas[L] @ a_prev.T) / m
    b_grads[L] = np.sum(deltas[L]) / m

    # Hidden Layers
    for l in range(L - 1, -1, -1):
        _, df = activations[act_funcs[l]]
        a_l = a[l]
        deltas[l] = (W[l + 1].T @ deltas[l + 1]) * df(a_l)
        a_prev = x if l == 0 else a[l - 1]
        W_grads[l] = (deltas[l] @ a_prev.T) / m
        b_grads[l] = np.sum(deltas[l]) / m

    return float(loss), W_grads, b_grads
         dataset = [
    ((0, 1, 0, 1, 0), 0),
    ((1, 0, 1, 0, 1), 1),
    ((1, 0, 1, 1, 0), 1),
    ((0, 1, 0, 0, 1), 1),
    ((1, 0, 0, 1, 1), 1),
    ((1, 0, 1, 1, 1), 0),
    ((1, 1, 1, 0, 1), 1),
    ((0, 0, 0, 0, 1), 0),
    ((1, 1, 1, 1, 0), 0),
    ((1, 1, 0, 0, 1), 1),
    ((1, 1, 1, 1, 1), 1),
    ((0, 0, 0, 0, 0), 1),
    ((1, 1, 1, 1, 1),1),
    ((0, 0, 0, 0, 0),1)
]

# Store the input data (X) and output (y) as numpy arrays
X = np.array([inp for inp, _ in dataset])   # shape (n_samples, n_features)
y = np.array([out for _, out in dataset])   # shape (n_samples,)

print(f"{'Input':<15} | {'Goal: y':<6}")
print("-" * 30)

for d, h in zip(X, y):
    print(f"{d!s:<15} | {h:<9}")

#Turn the rows into columns for later matrix multiplication!
X = X.T # shape (n_features, n_samples)
Y = y.T # shape (output_dim, n_samples)
act_funcs = ['tanh', 'sigmoid'] #Classic
#act_funcs = ['tanh', 'tanh', 'tanh']
#act_funcs = ['relu', 'sigmoid', 'sigmoid']

# Initialize Network:
#layer_sizes = [5, 3, 2, 1]
#layer_sizes = [5, 2, 1, 1]
#layer_sizes = [5, 1, 1, 1]
layer_sizes = [5, 2, 1]

print(len(act_funcs) == len(layer_sizes) - 1) #Make sure the number of activations match the number of layers
W, b = init_params(layer_sizes, seed=1337, scale=1.0)
print(W)
print(b)
alpha = 0.1
epochs = 10000

# Gradient Descent (original version)
for ep in range(1, epochs + 1):
    loss, W_grad, b_grad = forward_backward(X, Y, W, b, act_funcs)
    # Generate gradient values
    for l in range(len(W)):
      W[l] -= alpha * W_grad[l]
      b[l] -= alpha * b_grad[l]
    if ep % 1000 == 0:
        print(f"Epoch {ep:4d} | Loss: {loss:.6f}")

print("\nAfter training:")
output = forward(X, W, b, act_funcs)
print(output)
print("Rounded Predictions:")
print((output > 0.5).astype(int))
              print(f"{'Input':<20} | {'Goal: y':<10}| {'Output: y_hat':<15}")
print("-" * 60)

for d, h, out in zip(X.T, Y.T, output.T):
    print(f"{d!s:<20} | {h:<10}| {out[0]:<20} -> {round(out[0])}")
              # Test at a random point:
point = np.array([[0,0,0,0,0]]).T
forward(point, W, b, act_funcs)
    return W, b
