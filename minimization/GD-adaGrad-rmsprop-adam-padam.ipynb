import numpy as np
import matplotlib.pyplot as plt
#@title Helper Functions

import numpy as np

def heaviside(x: np.ndarray) -> np.ndarray:
    """
    Apply the heaviside function element-wise.

    Args:
        x: scalar or np.ndarray of floats

    Returns:
        np.ndarray of same shape as x, with values of 0 or 1
    """
    return np.array(x >= 0, dtype=int)

def sigmoid(x: np.ndarray) -> np.ndarray:
    """
    Apply the sigmoid function element-wise.

    σ(x) = 1 / (1 + exp(-x))

    Args:
        x: scalar or np.ndarray of floats

    Returns:
        np.ndarray of same shape as x, with values in (0,1)
    """
    return 1 / (1 + np.exp(-x))

def arctan(x: np.ndarray) -> np.ndarray:
    """
    Apply the ArcTan function element-wise.
    Note: ArcTan has been scaled to (-1, 1).

    φ(x) = (2/π) * arctan(x)

    Args:
        x: scalar or np.ndarray of floats

    Returns:
        np.ndarray of same shape as x, with values in (-1,1)
        
    """
    return (2/np.pi) * np.arctan(x)

#@title Various plotting functions to 'see' gradient descent in action

# Various functions to help generate some plots and 'see' gradient descent in action

def plot_contours_with_path(f, history, x_range=(-3,3), y_range=(-3,3), levels=30):
    xs = np.linspace(x_range[0], x_range[1], 400)
    ys = np.linspace(y_range[0], y_range[1], 400)
    X, Y = np.meshgrid(xs, ys)
    Z = np.vectorize(lambda a,b: f(np.array([a,b])))(X, Y)
    plt.figure(figsize=(6,5))
    CS = plt.contour(X, Y, Z, levels=levels)
    plt.clabel(CS, inline=True, fontsize=8)
    if history:
        xs_path = [h['x'] for h in history] + [history[-1]['x']]
        ys_path = [h['y'] for h in history] + [history[-1]['y']]
        plt.plot(xs_path, ys_path, marker='o')
    plt.xlabel('x'); plt.ylabel('y')
    plt.title(f"GD Path: {f_name}")
    plt.show()

def plot_contours_with_paths_multi(f, histories, labels, x_range=(-4,4), y_range=(-4,4), levels=30, title="Paths on Contours"):
    """
    histories: list of histories, each a list of dicts with keys 'x','y' (and typically 'iter','f',...)
    labels: list of strings, same length as histories
    """
    xs = np.linspace(x_range[0], x_range[1], 400)
    ys = np.linspace(y_range[0], y_range[1], 400)
    X, Y = np.meshgrid(xs, ys)
    Z = np.vectorize(lambda a,b: f(np.array([a,b])))(X, Y)

    plt.figure(figsize=(6,5))
    CS = plt.contour(X, Y, Z, levels=levels)
    plt.clabel(CS, inline=True, fontsize=8)

    for hist, label in zip(histories, labels):
        if not hist:
            continue
        x_path = [h['x'] for h in hist] + [hist[-1]['x']]
        y_path = [h['y'] for h in hist] + [hist[-1]['y']]
        plt.plot(x_path, y_path, marker='o', label=label)

    plt.xlabel('x'); plt.ylabel('y')
    plt.title(title)
    plt.legend(loc='lower left')
    #plt.legend()
    plt.show()

def plot_scalar_vs_iter(history, key, title):
    plt.figure(figsize=(6,4))
    iters = [h['iter'] for h in history]
    vals = [h[key] for h in history]
    plt.plot(iters, vals)
    plt.xlabel('Iteration'); plt.ylabel(key)
    plt.title(title)
    plt.grid(True)
    plt.show()

def plot_scalar_vs_iter_multi(histories, labels, key, title):
    plt.figure(figsize=(6,4))
    for hist, label in zip(histories, labels):
        if not hist:
            continue
        iters = [h['iter'] for h in hist]
        vals  = [h[key]    for h in hist]
        plt.plot(iters, vals, label=label)
    plt.xlabel('Iteration'); plt.ylabel(key)
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()
    #@title Various Optimizers (GD, Momentum, NAG, AdaGrad, RMSProp, ADAM, pADAM)

def gradient_descent(f, v: np.ndarray, alpha: float, max_iterations: int, grad_bound: float):
    #print(v)
    history = []

    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        v_new = v - alpha * g
        #print(v_new)
        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def gradient_descent_momentum(f, v: np.ndarray, alpha: float, beta: float, max_iterations: int, grad_bound: float):
    history = []
    vel = np.zeros_like(v)

    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        vel = beta * vel + alpha * g
        v_new = v - vel

        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha, 'beta': beta})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def gradient_descent_accelerated_gradient(f, v: np.ndarray, alpha: float, beta: float, max_iterations: int, grad_bound: float):
    # Nesterov Accelerated Gradient (NAG)
    history = []
    vel = np.zeros_like(v)

    for k in range(max_iterations):
        w = v - beta * vel
        g = approx_grad(w)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        vel = beta * vel + alpha * g
        v_new = v - vel

        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha, 'beta': beta})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def adagrad(f, v: np.ndarray, alpha: float, max_iterations: int, grad_bound: float):
    history = []
    vel = np.zeros_like(v)
    G = np.zeros_like(v, dtype=float)

    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        G += g**2 #G = G + g**2.
        alpha_t = alpha / (np.sqrt(G + 1e-8)) #Add a small value to ensure we do not divide by zero.

        vel = alpha_t * g
        v_new = v - vel

        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha_t})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def rmsprop(f, v: np.ndarray, alpha: float, rho: float, max_iterations: int, grad_bound: float):
    history = []
    vel = np.zeros_like(v)
    E = np.zeros_like(v, dtype=float)

    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        E = rho * E + (1.0 - rho) * (g**2)
        alpha_t = alpha / (np.sqrt(E + 1e-8))

        vel = alpha_t * g
        v_new = v - vel

        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha, 'beta': beta})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def adam(f, v: np.ndarray, alpha: float, beta1: float, beta2: float, max_iterations: int, grad_bound: float):
    history = []
    E1 = np.zeros_like(v, dtype=float)
    E2 = np.zeros_like(v, dtype=float)

    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        E1 = beta1 * E1 + (1.0 - beta1) * g
        E2 = beta2 * E2 + (1.0 - beta2) * (g**2)

        E1_hat = E1 / (1.0 - beta1 ** (k + 1))
        E2_hat = E2 / (1.0 - beta2 ** (k + 1))
        alpha_t = alpha / (np.sqrt(E2_hat + 1e-8))

        vel = alpha_t * E1_hat
        v_new = v - vel

        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha, 'beta': beta})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}

def padam(f, v: np.ndarray, p: int, alpha: float, beta1: float, beta2: float, max_iterations: int, grad_bound: float):
    history = []
    E1 = np.zeros_like(v, dtype=float)
    EP = np.zeros_like(v, dtype=float)

    for k in range(max_iterations):
        g = approx_grad(v)
        g_norm = float(np.linalg.norm(g))
        f_v = float(f(v))

        E1 = beta1 * E1 + (1.0 - beta1) * g
        EP = beta2 * EP + (1.0 - beta2) * (abs(g)**p)

        E1_hat = E1 / (1.0 - beta1 ** (k + 1))
        EP_hat = EP / (1.0 - beta2 ** (k + 1))
        alpha_t = alpha / (np.sqrt(EP_hat + 1e-8))

        vel = alpha_t * E1_hat
        v_new = v - vel

        history.append({'iter': k, 'x': v[0], 'y': v[1], 'f': f_v, 'grad_norm': g_norm, 'alpha': alpha, 'beta': beta})
        v = v_new
        if g_norm < grad_bound:
            break
    return {'v_opt': v, 'f_opt': float(f(v)), 'history': history}
    dataset = [
    ((0,0),0),
    ((0,1),0),
    ((1,0),0),
    ((1,1),1)
]

print(f"{'Input':<7} | {'Output':<6}")
print("-" * 17)

for d, h in dataset:
    print(f"{d!s:<7} | {h:<6}")
    # Store the input data (X) and output (y) as numpy arrays
X = np.array([inp for inp, _ in dataset], dtype=int)   # shape (n_samples, n_features)
y = np.array([out for _, out in dataset], dtype=int)   # shape (n_samples,)

# Here y is the desired output (what comes out of the heaviside or sigmoid). We can generate a pre-activation z such that h(z) = y.
z = 2*y - 1

print(f"{'Input':<5} | {'Output: y':<6} | {'Pre-Activation: z':<15}")
print("-" * 38)

for d, h, p in zip(X, y, z):
    print(f"{d!s:<5} | {h:<9} | {p:<17}")
    # We can add a stack of 1s to the end of X to generate X with bias column
X_b = np.hstack([X, np.ones((X.shape[0], 1), dtype=int)])

print("X_b (X with bias column):\n", X_b)
w_b = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ z
print('w_b =',w_b)
#We can check to see if X_b.w_b is equal to z
temp_z = X_b @ w_b
print(temp_z == z)

print(f"{'Input':<11} | {'Model Output (z)':<20} | {'Goal Output (z)':<10}")
print("-" * 53)

for d,a,b in zip(X,temp_z, z):
    print(f"{d!s:<11} | {a:<20} | {b:<10}")
    #We can check to see if h(X_b.w_b) is equal to h(z) = y
temp_z = X_b @ w_b
temp_y_heaviside = heaviside(temp_z)
temp_y_sigmoid = sigmoid(temp_z)

print(f"{'Input':<11} | {'Model Output Heaviside (y)':<26} | {'Model Output Sigmoid (y)':<26} |{' Goal Output (y)':<10}")
print("-" * 84)

for d,a,b,c in zip(X,temp_y_heaviside, temp_y_sigmoid, y):
    print(f"{d!s:<11} | {a:<26} | {b:<26} | {c:<16}")
# Now we can try to minimize f(w0,w1,b) = ||Xb @ wb - z||^2.

def f(v: np.ndarray):
    w0, w1, b = v
    return (np.sum((Xb @ v - z)**2))
    #return (w0 - w1 - b)**2 #<- Update this function to match the error functions.
f_name = 'f(w0,w1,b) = ||Xb @ wb - z||^2'

#You can define exact grad_f if desired.

def approx_grad(v: np.ndarray, h = 1e-10):
    """
    Computes approximations of f_x(a,b) and f_y(a,b) via
    f_x(a,b) ~ (f(a+h,b)-f(a-h,b))/(2h)
    f_y(a,b) ~ (f(a,b+h)-f(a-h,b))/(2h)

    returns [f_x(a,b),f_y(a,b)]
    """
    g = np.zeros_like(v) #Initialize g as zero vector
    for i in range(len(v)):
        step = np.zeros_like(v, dtype=float)
        step[i] = h
        #Step is of the form (0,0,..,0,h,0,...,0) with h in the i index
        g[i] = (f(v+step) - f(v-step))/(2*h) #Symmetric Approximation
    return g
    v0 = np.array([1,1,1]) #Initial Point
alpha = 0.05 #Learning Rate
beta = 0.9 #Momentum Rate
rho = 0.9 #RMSProp Rate (RMSProp)
beta1 = 0.7 #Decay Rate (ADAM)
beta2 = 0.72 #Decay Rate (ADAM)
max_iterations = 2000
grad_bound = 1e-6

gd_res        = gradient_descent(f, v0, alpha, max_iterations, grad_bound)
mom_res       = gradient_descent_momentum(f, v0, alpha, beta, max_iterations, grad_bound)
ada_res       = adagrad(f, v0, alpha, max_iterations, grad_bound)
rms_res       = rmsprop(f, v0, alpha, rho, max_iterations, grad_bound)
adam_res      = adam(f, v0, alpha, beta1, beta2, max_iterations, grad_bound)
adam_1_res    = padam(f, v0, 1, alpha, beta1, beta2, max_iterations, grad_bound)
adam_3_res    = padam(f, v0, 3, alpha, beta1, beta2, max_iterations, grad_bound)

histories = [gd_res['history'], mom_res['history'], ada_res['history'], rms_res['history'], adam_res['history'], adam_1_res['history'], adam_3_res['history']]
labels    = ['GD', 'Momentum', 'AdaGrad', 'RMSProp', 'ADAM', 'ADAM (p=1)', 'ADAM (p=3)']
# Gradient Descent Results (gd_res)
v_output, f_output = gd_res['v_opt'], gd_res['f_opt']
history = gd_res['history']
print(f"v* = {v_output}, f(v*) = {f_output}")
print(f"Iterations: {len(history)}")
# ADAM Results (adam_res)
v_output, f_output = adam_res['v_opt'], adam_res['f_opt']
history = adam_res['history']
print(f"v* = {v_output}, f(v*) = {f_output}")
print(f"Iterations: {len(history)}")

    
